[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Status: Work in progress\nWorking title: “An adversarial collaboration contrasting two explanations for the effect of working memory load on distractor processing during encoding and retention”\nAuthors: David Greeno, Candice C. Morey and Vencislav Popov\nLinks:\n\nProject website\nPreregistration\nGitHub repository\n\n\n\n\n\n\n\nImportant\n\n\n\nThis project uses a workflow that is designed to be reproducible on any machine and at any time. The code is organized as an R project, and the project uses the renv package to manage the R package dependencies, the targets R package to manage the execution of the workflow, and the Quarto software to write and render analysis reports as notebooks published on this website. If you are not familiar with these tools, you can follow the steps below to download the project and reproduce the analyses.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#project-information",
    "href": "index.html#project-information",
    "title": "About",
    "section": "",
    "text": "Status: Work in progress\nWorking title: “An adversarial collaboration contrasting two explanations for the effect of working memory load on distractor processing during encoding and retention”\nAuthors: David Greeno, Candice C. Morey and Vencislav Popov\nLinks:\n\nProject website\nPreregistration\nGitHub repository\n\n\n\n\n\n\n\nImportant\n\n\n\nThis project uses a workflow that is designed to be reproducible on any machine and at any time. The code is organized as an R project, and the project uses the renv package to manage the R package dependencies, the targets R package to manage the execution of the workflow, and the Quarto software to write and render analysis reports as notebooks published on this website. If you are not familiar with these tools, you can follow the steps below to download the project and reproduce the analyses.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-download-and-reproduce-the-analyses",
    "href": "index.html#how-to-download-and-reproduce-the-analyses",
    "title": "About",
    "section": "How to download and reproduce the analyses",
    "text": "How to download and reproduce the analyses\n\nStep 0: Install R and RStudio\nTo reproduce the analyses in this project, you will need to have R1 and maybe RStudio installed on your computer. You can download R from the Comprehensive R Archive Network (CRAN), and RStudio from the RStudio website. If you are not using RStudio, you need to also install Quarto to render the quarto reports (this website).\n\n\nStep 1: Download the repository\nYou can download the repository as a zip file2 and extract it on your computer, or you can clone the repository using git by running the following command in your terminal3:\ngit clone https://github.com/venpopov/complexSpanAdversarial.git\nNext, navigate to the project directory:\ncd complexSpanAdversarial\n\n\nStep 2: Open the project in RStudio or another IDE\nThe project is structured so that you can replicate it on any machine without having to worry about R package versions, file paths4, etc. Once you have R and RStudio installed, you can open the complexSpanAdversarial.Rproj file in RStudio, which will set the working directory to the base directory of the project.\n\n\nStep 3: Install renv and the necessary R packages\nWe use the renv package to record the specific R packages and their versions necessary for this project. When you open the project in RStudio, renv will first install itself (if it is not already installed). If for some reason the renv package is not installed, you can install it by running install.packages(\"renv\") in the R console.\nAfter the renv package is installed, you can install all the packages used in the project by running renv::restore(). This will install all the packages listed in the renv.lock file in a separate library specific to this project.\n\n\nStep 4: Run the workflow via the targets package\nWe use the targets package to manage the code execution of the project. All the functionality of the project is organized in the R folder as custom functions which prepare the data, run the analyses, and generate the reports. The targets package is used to manage the execution of these functions in the correct order. The workflow is defined in the _targets.R file in the base directory of the project.\nYou can run the entire workflow by running targets::tar_make(). This will generate all computational outputs in the project, which can then be used to generate the reports or do further analyses.\n\n\n\n\n\n\nTips for using targets\n\n\n\nThe targets package improves reproducibility, but if you are not familiar with it, it can be a bit confusing and can obscure the code execution. Here are some tips for using targets:\n\neach tar_target function in the _targets.R file creates an R object that is saved in the targets folder\ntar_make() runs the entire workflow and generates all the outputs. This can take some time, especially if the outputs are large. Once the outputs are generated, you can run tar_make() again and it will only run the targets that have changed since the last run\nwhen working interactively or writing additional analyses in notebooks, you can load the output of each target by running tar_load(target_name) or tar_load_all() (the latter can be slow if there are many targets)\ntar_source() sources all files under the R folder. If you want to run a specific function, you can source the file and run the function directly\n\n\n\n\n\nStep 5: Render the reports\nThe reports are written in the Quarto format and are published on the project website. You can render the reports locally via one of the following methods:\n\nrun quarto render in the terminal (not the R console!) from the base directory of the project to render all the reports as html files to the _site folder\nrun quarto render docs/some_report.qmd to render a specific report (replace some_report.qmd with the name of the report you want to render)\nrun quarto::render() in the R console to render all reports from R\nclick the “Build &gt; Render website” button in RStudio to render all reports or the Render button to render a specific report you are currently viewing",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#folder-structure",
    "href": "index.html#folder-structure",
    "title": "About",
    "section": "Folder structure",
    "text": "Folder structure\nThe project is organized in the following main folders:\n\ndata-raw: raw data files\noutput: .rds files containing computational outputs\ndocs: source files for the quarto reports\nR: custom R functions (no interactive code)",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#website-navigation",
    "href": "index.html#website-navigation",
    "title": "About",
    "section": "Website navigation",
    "text": "Website navigation\nYou can find the rendered reports on the project website. The website is organized in the following sections:\n\nDevelopment notes: an unorganized collection of notebooks. These are mostly for my own reference during model development.\nNotebooks: a collection of notebooks that are more organized and presentable. These are intended to be shared with others.\nReports: a collection of reports that summarize the development of the model and the results of the model.\nFunction reference: documentation of custom functions used in the project.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe project was developed using R version 4.4.0. If you are using a different version of R, you may encounter issues with package compatibility.↩︎\nClick on the green “Code” button on the repository page, then selecting “Download ZIP”↩︎\nThis assumes you have git installed on your computer. If you don’t have git installed, you can download it from the git website.↩︎\nThe project uses relative paths, so you should be able to replicate it on any machine without having to worry about file paths. If you are not familiar with working with code structured as a project, see here.↩︎",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "docs/archive/index.html",
    "href": "docs/archive/index.html",
    "title": "Archived scripts",
    "section": "",
    "text": "Note\n\n\n\nThese are legacy analyses scripts from my older project structure. The scripts are kept here for reference and reproducibility purposes until I completely transition to the new structure. I want to replicate the analysis in the new project structure to make sure everything is working as expected. Nowadays I also have better solutions than if(file.exists()) for managing the data and model fits, so I will update the script accordingly.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Archive"
    ]
  },
  {
    "objectID": "docs/archive/resource_model_description.html",
    "href": "docs/archive/resource_model_description.html",
    "title": "Initial model predictions",
    "section": "",
    "text": "Note\n\n\n\nThese are legacy analyses scripts from my older project structure. The scripts are kept here for reference and reproducibility purposes until I completely transition to the new structure. I want to replicate the analysis in the new project structure to make sure everything is working as expected. Nowadays I also have better solutions than if(file.exists()) for managing the data and model fits, so I will update the script accordingly.\nCode\nlibrary(tidyverse)\nlibrary(rtdists)\nlibrary(here)",
    "crumbs": [
      "Home",
      "Archive",
      "Initial model predictions"
    ]
  },
  {
    "objectID": "docs/archive/resource_model_description.html#explanation-of-the-model-and-model-code",
    "href": "docs/archive/resource_model_description.html#explanation-of-the-model-and-model-code",
    "title": "Initial model predictions",
    "section": "Explanation of the model and model code",
    "text": "Explanation of the model and model code\nThis is a simplified version of the resource model. All results below are simulated from the following functions. The model works as follows. Any WM trial begins with maximum resources R=1. The encoding of items and distractors each deplete a certain proportion of the currently available resources. Encoding of items depletes pMem proportion of resources, while the encoding of distractors depletes pTone proportion of resources, where pTone is assumed to be less than pMem. Resources recover as a linear function of time with rate of rate resources/second. The strength of the encoded item or tone is equal to pMem * R_available and of tones to pTone * R_available. For example, we start with R=1, and then the first item depletes 50% of available resources (pMem = 0.5), so it’s strength is 0.5*1=0.5. Then a tone is presented is 1.5 sec after the memory item onset. In that time resources have recovered to 0.5 + 1.5*0.1= 0.65. The tone depletes 0.1*0.65=0.065 resources, and leaves 0.585 available, etc. Because each memory item depletes more resources than what can be recovered during the ISI, each “load” tone is encoded less and less well. In contrast, during the retention interval no memory items are encoding, and the encoding rate of tone items is less than the recovery rate, so each subsequent tone is encoded more strongly. Finally, tone judgement RTs are modeled as a drift diffusion process with the drift rate being proportional to the encoding strength of the tone item. The only adjustment to RTs that doesn’t come from the model is a constant added to the initial lead-in judgement to reflect switching costs. Everything else is simulated from the model.\nThe function below is given as an input the three learning and recovery parameters, then the details of the study procedure (number of leadin items, setsize, number of lead out items, timing parameters for the presentation) and a couple of scaling parameters for the diffusion model. Then it outputs memory strength values for each memory item and each probe item, as well as RT predictions for the tone judgements.\nI did not fit the model to the data, but rather chose a set of parameters that give a good approximation (for simplicity; for the eventual paper I should fit the model properly).\n\n\nCode\nsim_data &lt;- function(pMem=0.5, pTone=0.1, rate=0.05, \n                     leadin_n = 4, \n                     setsize = 7,\n                     leadout_n = 8,\n                     mem_duration = 1,\n                     mem_tone_ISI = 0.5,\n                     tone_duration = 0.75+0.25,\n                     decision_time = 0.8,\n                     tone_mem_ISI = 0.5,\n                     a=2, vc=40) {\n  \n  # get_resources &lt;- function(R_prev, p, rate, ti) {\n  #   R = 1 - (1-(1-p)*R_prev)*exp(-rate*ti) \n  #   return(R)\n  # }\n  \n  # function calculating amount of remaining resources after depletion and recovery\n  get_resources_lin &lt;- function(R_prev, p, rate, ti) {\n    R = sapply(ti, function(x) min(1, (1-p)*R_prev + rate*x))\n    return(R)\n  }\n  \n  # function constructs a sequence of expeirmental events based on experimental parameters\n  gen_exp_info &lt;- function() {\n    trial_seq = c(rep('tone', times=leadin_n),\n                  rep(c('mem','tone'), times=setsize), \n                  rep('tone', times=leadout_n))\n    ps = ifelse(trial_seq == \"mem\", pMem, pTone)\n    trial_times = c(0,\n                    rep(tone_duration+decision_time+tone_mem_ISI, times=leadin_n),\n                    rep(c(mem_duration+mem_tone_ISI, tone_duration+decision_time+tone_mem_ISI), times=setsize), \n                    rep(tone_duration+decision_time+tone_mem_ISI, times=leadout_n))\n    return(list(trial_seq=trial_seq, ps=ps, trial_times=trial_times))\n  }\n  \n  # function generates encoding strength values based on learning parameters and amount of available resources\n  gen_strength &lt;- function() {\n    \n    strengths = c()\n    Rs = c()\n    R = 1\n    for (i in 1:length(exp_info$trial_seq)) {\n      p_prev = ifelse(i==1, 0, exp_info$ps[i-1])\n      R = get_resources_lin(R, p_prev, rate, exp_info$trial_times[i])\n      Rs = c(Rs, R)\n      strengths &lt;- c(strengths, R*exp_info$ps[i])\n      i=i+1\n    }\n    return(list(strengths=strengths, Rs=Rs))\n  }\n  \n  \n  extract_tone_strengths &lt;- function(strengths) {\n    strengths[exp_info$trial_seq=='tone']\n  }\n  \n  # generate RTs based on a drift diffusion process from the encoding strnegth as a drift rate.\n  gen_tone_rts &lt;- function(strengths) {\n    sapply(strengths, function(x) qdiffusion(0.5, a=a, v=vc*x, t0=0.2))\n  }  \n  \n  \n  ## run simulation with default parameter\n  exp_info = gen_exp_info()\n  sim = gen_strength()\n  all_s = sim$strengths\n  tone_s = extract_tone_strengths(all_s)\n  rts = gen_tone_rts(tone_s)\n  pars = list(pMem=pMem, pTone=pTone, rate=rate,\n              leadin_n=leadin_n, setsize=setsize, leadout_n=leadout_n,\n              mem_duration=mem_duration, mem_tone_ISI = mem_tone_ISI,\n              tone_duration=tone_duration, decision_time=decision_time,\n              tone_mem_ISI = tone_mem_ISI)\n  return(list(exp_info=exp_info, all_s=all_s, tone_s=tone_s, rts=rts, Rs=sim$Rs, pars=pars))\n}",
    "crumbs": [
      "Home",
      "Archive",
      "Initial model predictions"
    ]
  },
  {
    "objectID": "docs/archive/resource_model_description.html#simulations-of-existing-data",
    "href": "docs/archive/resource_model_description.html#simulations-of-existing-data",
    "title": "Initial model predictions",
    "section": "Simulations of existing data",
    "text": "Simulations of existing data\nHere’s the overall initial simulation to show that the model captures the pattern presented in Joseph and Morey (2021). Memory items deplete 30% of resources, tones deplete 10% of resources, and the resource recovers at a rate of 0.05 per second. The model simulation captures all major aspects of the data.\n\n\nCode\nsimdat &lt;- sim_data(pMem=0.3, pTone=0.1, rate=0.05, a=3, v=40)\ndat &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('Load-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('Load', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s)\ndat$pos = 1:nrow(dat)\n\ndat$rts[1] = dat$rts[1]+0.3\n\n\nggplot(dat, aes(pos, rts, group=type)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(\"Serial position\", labels=dat$type_pos, breaks=1:nrow(dat)) +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5)) +\n  scale_y_continuous(\"Mean Response Time\")\n\n\n\n\n\n\n\n\n\nCode\nggsave(here('figures/sac_jm2021_simulation.png'), width=6, height=4, units='in')\n\n\nThe model also predicts that the lead-out judgements should be overall faster with smaller setsizes, as shown in Figure 6 of Joseph and Morey (2021)\n\n\nCode\ndat &lt;- data.frame()\nfor (setsize in c(4,6)) {\n  simdat &lt;- sim_data(pMem=0.3, rate=0.06, a=4, v=40, leadout_n=4,  setsize=setsize)\n  DAT &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('Load-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('Load', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  pos = c(-sort(1:simdat$pars$leadin_n, decreasing = T),\n                          1:simdat$pars$setsize,\n                          8:(7+simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s,\n                  setsize = simdat$pars$setsize)\n  DAT$rts[1] = DAT$rts[1]+0.3\n  dat &lt;- bind_rows(dat,DAT)\n}\n\n\n\nggplot(dat, aes(pos, rts, group=interaction(type, setsize), color=as.factor(setsize))) +\n  geom_point() +\n  geom_line() +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5)) +\n  coord_cartesian(ylim=c(0.6,1.3))",
    "crumbs": [
      "Home",
      "Archive",
      "Initial model predictions"
    ]
  },
  {
    "objectID": "docs/archive/resource_model_description.html#predictions-for-the-running-span-experiment",
    "href": "docs/archive/resource_model_description.html#predictions-for-the-running-span-experiment",
    "title": "Initial model predictions",
    "section": "Predictions for the running span experiment",
    "text": "Predictions for the running span experiment\nIn the model the running span task is implemented during encoding exactly the same as the simple span task - each item depletes resources regardless of whether the items is in position less or larger than the recall span. The model predicts that RTs gradualy get less and less of decrement with each serial position, but they never completely asymptote (at least not with up to 10 items). On the plot below there is only one line during the load judgements because the smaller setsizes lines overlap completely.\n\n\nCode\ndat &lt;- data.frame()\nfor (setsize in c(4,6,8,10)) {\n  simdat &lt;- sim_data(pMem=0.3, a=3, v=40, leadout_n=8,  setsize=setsize)\n  DAT &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('Load-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('Load', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  pos = c(-sort(1:simdat$pars$leadin_n, decreasing = T),\n                          1:simdat$pars$setsize,\n                          11:(10+simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s,\n                  setsize = simdat$pars$setsize)\n  DAT$rts[1] = DAT$rts[1]+0.3\n  dat &lt;- bind_rows(dat,DAT)\n}\n\n\n\nggplot(dat, aes(reorder(type_pos,pos), rts, group=interaction(type, setsize), color=as.factor(setsize))) +\n  geom_point() +\n  geom_line() +\n  # scale_x_continuous(labels=type_pos, breaks=1:nrow(dat)) +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat &lt;- data.frame()\nfor (setsize in c(2:7)) {\n  simdat &lt;- sim_data(pMem=0.3, a=3, v=40, leadout_n=8,  setsize=setsize)\n  DAT &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('List-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('List', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  pos = c(-sort(1:simdat$pars$leadin_n, decreasing = T),\n                          1:simdat$pars$setsize,\n                          11:(10+simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s,\n                  setsize = simdat$pars$setsize)\n  DAT$rts[1] = DAT$rts[1]+0.3\n  dat &lt;- bind_rows(dat,DAT)\n}\n\n\n\ndat %&gt;% \n  filter(type != \"Lead-out\") %&gt;% \n  ggplot(aes(reorder(type_pos,pos), rts*1000, group=interaction(type, setsize), color=as.factor(setsize))) +\n  geom_point() +\n  geom_line() +\n  # scale_x_continuous(labels=type_pos, breaks=1:nrow(dat)) + +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5)) +\n  scale_y_continuous(\"Mean Response Time\") +\n  scale_color_discrete('List Length') +\n  scale_x_discrete('Serial Position')\n\n\n\n\n\n\n\n\n\nCode\nggsave(here('figures/resource_predictions.png'), width=5, height=3, units='in')\n\n\nOne potential complication we discussed is that Klaus has found that he can model the running span task with a resource recovery model if he assumes that people encode items in the running span task more passively, which results in a lower depletion rate (i.e., smaller pMem parameter). I next ran the model with a range of pMem values, to make sure that the same predictions are carried out regardless if people deplete less resources in the running span task. The next simulation varies pMem from 0.15 to 0.35. We can see that this shifts the RTs up and down but doesn’t change the shape of the function much. With lower learning rate, there is less of a decrement on tone RTs, but tone RTs still slow down with each item added to memory. The effect will only disappear if the learning rate gets down to 0.08\n\n\nCode\ndat &lt;- data.frame()\nfor (pmem in c(0.08, 0.15, 0.2,0.25, 0.3, 0.35)) {\n  simdat &lt;- sim_data(pMem=pmem, a=3, v=40, leadout_n=8,  setsize=10)\n  DAT &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('Load-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('Load', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  pos = c(-sort(1:simdat$pars$leadin_n, decreasing = T),\n                          1:simdat$pars$setsize,\n                          11:(10+simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s,\n                  pmem = pmem)\n  DAT$rts[1] = DAT$rts[1]+0.3\n  dat &lt;- bind_rows(dat,DAT)\n}\n\n\n\nggplot(dat, aes(reorder(type_pos,pos), rts, group=interaction(type, pmem), color=as.factor(pmem))) +\n  geom_point() +\n  geom_line() +\n  # scale_x_continuous(labels=type_pos, breaks=1:nrow(dat)) +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5))",
    "crumbs": [
      "Home",
      "Archive",
      "Initial model predictions"
    ]
  },
  {
    "objectID": "docs/archive/resource_model_description.html#predictions-about-isi-experiment",
    "href": "docs/archive/resource_model_description.html#predictions-about-isi-experiment",
    "title": "Initial model predictions",
    "section": "Predictions about ISI experiment",
    "text": "Predictions about ISI experiment\nThe model predicts that if we increase either the item-to-tone ISI, or the tone-to-item ISI, we should get less slow down with each item, because resources would have recovered more:\n\n\nCode\ndat &lt;- data.frame()\nfor (memtoneISI in c(0.1,0.3,0.5,0.7,0.9,1.5,2)) {\n  simdat &lt;- sim_data(pMem=0.3, a=3, v=40, leadout_n=8,  setsize=10, mem_tone_ISI = memtoneISI)\n  DAT &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('Load-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('Load', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  pos = c(-sort(1:simdat$pars$leadin_n, decreasing = T),\n                          1:simdat$pars$setsize,\n                          11:(10+simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s,\n                  mem_tone_ISI = memtoneISI)\n  DAT$rts[1] = DAT$rts[1]+0.3\n  dat &lt;- bind_rows(dat,DAT)\n}\n\n\n\nggplot(dat, aes(reorder(type_pos,pos), rts, group=interaction(type, mem_tone_ISI), color=as.factor(mem_tone_ISI))) +\n  geom_point() +\n  geom_line() +\n  # scale_x_continuous(labels=type_pos, breaks=1:nrow(dat)) +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat &lt;- data.frame()\nfor (tonememISI in c(0.1,0.3,0.5,0.7,0.9,1.5,2)) {\n  simdat &lt;- sim_data(pMem=0.3, a=3, v=40, leadout_n=8,  setsize=10, tone_mem_ISI = tonememISI)\n  DAT &lt;- data.frame(type_pos = c(paste0('Lead-in-',sort(1:simdat$pars$leadin_n, decreasing = T)),\n                               paste0('Load-', 1:simdat$pars$setsize),\n                               paste0('Lead-out-', 1:simdat$pars$leadout_n)),\n                  type = c(rep('Lead-in', simdat$pars$leadin_n),\n                           rep('Load', simdat$pars$setsize),\n                           rep('Lead-out', simdat$pars$leadout_n)),\n                  pos = c(-sort(1:simdat$pars$leadin_n, decreasing = T),\n                          1:simdat$pars$setsize,\n                          11:(10+simdat$pars$leadout_n)),\n                  rts = simdat$rts,\n                  strength = simdat$tone_s,\n                  tone_mem_ISI = tonememISI)\n  DAT$rts[1] = DAT$rts[1]+0.3\n  dat &lt;- bind_rows(dat,DAT)\n}\n\n\n\nggplot(dat, aes(reorder(type_pos,pos), rts, group=interaction(type, tone_mem_ISI), color=as.factor(tone_mem_ISI))) +\n  geom_point() +\n  geom_line() +\n  # scale_x_continuous(labels=type_pos, breaks=1:nrow(dat)) +\n  theme(axis.text.x = element_text(angle=60, vjust=0.5))\n\n\n\n\n\n\n\n\n\nThe only difference is that if we increase the item-to-tone ISI, we also get an effect on the first tone, but we increase the tone-to-item ISI we only get effects on the subsequent tones. In contrast, I think that the reconfiguration hypothesis would predict that only increasing the mem-to-tone ISI should decrease the slow-down rate of tone_judgements with each serial positions, but that there should be no effect of the tone_to_mem ISI.",
    "crumbs": [
      "Home",
      "Archive",
      "Initial model predictions"
    ]
  },
  {
    "objectID": "docs/notes.html",
    "href": "docs/notes.html",
    "title": "Notes",
    "section": "",
    "text": "Initialize the repository. I’m setting up my new project structure with Quarto for reporting, renv for package management, and GitHub for version control, and targets for workflow management. There are already some notebooks and analysis outputs in the previous project structure. For now I’m going to copy them over to the new structure and start working on the new simulations and analyses. Before publishing however, I should replicate the original notebooks and outputs in the new structure to make sure everything is working as expected.\n\n\n\nadd the original raw data after replacing the prolific ids with random strings (data-raw/anonymize_data.R)\nadd preprocessed data and brms output from previous analysis in output/exp1_updated.RData and output/exp2.RData\n\nTODO: these are big files, so cannot be shared on github. Should upload them to OSF and download them via script to the project folder when someone clones the repository\n\n\n\n\n\n\nAsk Candice about notebook for generating her predictions\nIntroduce the idea behind the adversarial collaboration on the intro page",
    "crumbs": [
      "Home",
      "Development notes",
      "Notes"
    ]
  },
  {
    "objectID": "docs/notes.html#log",
    "href": "docs/notes.html#log",
    "title": "Notes",
    "section": "",
    "text": "add the original raw data after replacing the prolific ids with random strings (data-raw/anonymize_data.R)\nadd preprocessed data and brms output from previous analysis in output/exp1_updated.RData and output/exp2.RData\n\nTODO: these are big files, so cannot be shared on github. Should upload them to OSF and download them via script to the project folder when someone clones the repository",
    "crumbs": [
      "Home",
      "Development notes",
      "Notes"
    ]
  },
  {
    "objectID": "docs/notes.html#tasks",
    "href": "docs/notes.html#tasks",
    "title": "Notes",
    "section": "",
    "text": "Ask Candice about notebook for generating her predictions\nIntroduce the idea behind the adversarial collaboration on the intro page",
    "crumbs": [
      "Home",
      "Development notes",
      "Notes"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-1ab-Ven.html",
    "href": "docs/archive/JointAnalyses-1ab-Ven.html",
    "title": "Joint analysis of E1a and E1b",
    "section": "",
    "text": "Note\n\n\n\nThese are legacy analyses scripts from my older project structure. The scripts are kept here for reference and reproducibility purposes until I completely transition to the new structure. I want to replicate the analysis in the new project structure to make sure everything is working as expected. Nowadays I also have better solutions than if(file.exists()) for managing the data and model fits, so I will update the script accordingly.",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E1a and E1b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-1ab-Ven.html#load-and-clean-data",
    "href": "docs/archive/JointAnalyses-1ab-Ven.html#load-and-clean-data",
    "title": "Joint analysis of E1a and E1b",
    "section": "Load and clean data",
    "text": "Load and clean data\nJoint analysis of Experiment 1a and 1b. Load libraries:\n\n\nCode\nlibrary(lme4)\nlibrary(Rmisc)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(brms)\n\n\ndodge &lt;- position_dodge(1)\ntheme_set(theme_dark(base_size = 11))\n\n\nLoad and combine data that was preprocessed with David’s script.\n\n\nCode\n# if the script has been run before, preload the data and model fits\nR_output_file &lt;- here::here('output/exp1_updated.RData')\n\nif (!file.exists(R_output_file)) {\n  exp1a &lt;- read.csv(here::here('data/Expmt1a-PreReg/Processed/LongFormAnalysisData.csv'))\n  exp1b &lt;- read.csv(here::here('data/Expmt1b-PreReg/Processed/LongFormAnalysisData.csv'))\n  exp1a$exp &lt;- '1a'\n  exp1b$exp &lt;- '1b'\n  exp1b$participant &lt;- exp1b$participant\n  dat &lt;- bind_rows(exp1a,exp1b)\n  \n  # code sample (students vs prolific)\n  dat &lt;- dat %&gt;% \n    mutate(sample = ifelse(nchar(participant) &lt; 6, 'students','prolific'))\n  \n  # Apply David's trimming script\n  procTrim &lt;- dat %&gt;% \n    filter(ProportionTonesCorrect == 1) %&gt;% \n    mutate(rt = rt*1000) %&gt;%  # Make RTs milliseconds\n    group_by(participant) %&gt;% # grouping by participant so SD per participant calculated - similar to perParticipant = True in Trimr\n    filter(rt &gt;= 200)%&gt;%      # remove anything &lt;200ms as this likely means they were anticipating response rather than reacting - same as Joseph & Morey 2021\n    filter(rt &lt; 5000) %&gt;%       \n    filter(abs(rt - mean(rt))/sd(rt) &lt;= 3) \n  \n  # This just gives a quick summary count of amount of data before and after trimming.\n  nrow(dat) #original number\n  nrow(procTrim) #number remaining\n  nrow(dat) - nrow(procTrim) #number removed  \n  \n  ##Rename variables to align with previous research\n  procTrim[procTrim == \"LeadTone_RT1\"] &lt;- \"Lead-in-4\"\n  procTrim[procTrim == \"LeadTone_RT2\"] &lt;- \"Lead-in-3\"\n  procTrim[procTrim == \"LeadTone_RT3\"] &lt;- \"Lead-in-2\"\n  procTrim[procTrim == \"LeadTone_RT4\"] &lt;- \"Lead-in-1\"\n  procTrim[procTrim == \"MainTone_RT1\"] &lt;- \"List-1\"\n  procTrim[procTrim == \"MainTone_RT2\"] &lt;- \"List-2\"\n  procTrim[procTrim == \"MainTone_RT3\"] &lt;- \"List-3\"\n  procTrim[procTrim == \"MainTone_RT4\"] &lt;- \"List-4\"\n  procTrim[procTrim == \"MainTone_RT5\"] &lt;- \"List-5\"\n  procTrim[procTrim == \"MainTone_RT6\"] &lt;- \"List-6\"\n  procTrim[procTrim == \"MainTone_RT7\"] &lt;- \"List-7\"\n  procTrim[procTrim == \"MainTone_RT8\"] &lt;- \"List-8\"\n  procTrim[procTrim == \"MainTone_RT9\"] &lt;- \"List-9\"\n  \n  procTrim &lt;- procTrim %&gt;%  \n    mutate(procPeriod = case_when(grepl(\"Lead-in\", procPos) ~ \"Lead-in\",\n                                  grepl(\"List\", procPos) ~\"Load\")) %&gt;% \n    mutate(procPos = factor(procPos, levels = c(\"Lead-in-4\", \"Lead-in-3\", \"Lead-in-2\", \"Lead-in-1\", \"List-1\", \"List-2\", \"List-3\", \"List-4\", \"List-5\", \"List-6\", \"List-7\", \"List-8\", \"List-9\"))) %&gt;%\n    mutate(ListLength = as.factor(ListLength))%&gt;%\n    mutate(serial_position_abs = as.numeric(gsub('List-','', procPos)),\n           serial_position_rel = serial_position_abs-recallamount) %&gt;% \n    mutate(recallamount = as.factor(recallamount)) %&gt;% \n    ungroup()\n\n} else {\n  load(R_output_file)\n  R_output_file &lt;- here::here('output/exp1_updated.RData')\n}",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E1a and E1b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-1ab-Ven.html#data-visualisation",
    "href": "docs/archive/JointAnalyses-1ab-Ven.html#data-visualisation",
    "title": "Joint analysis of E1a and E1b",
    "section": "Data Visualisation",
    "text": "Data Visualisation\nWe can now plot our tidied and trimmed data. We will relabel some variables and then calculate the summary statistics before plotting.\nReproduce the basic plot for both experiments (code from David’s script)\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- summarySEwithin(ungroup(procTrim), measurevar = \"rt\", withinvars = c(\"ListLength\", \"recallamount\", \"procPos\", \"procPeriod\"), betweenvars = \"exp\", idvar = \"participant\")\n\n\nAutomatically converting the following non-factors to factors: exp, procPeriod\n\n\nCode\nggplot(graphRT, aes(procPos, rt, group = interaction(procPeriod, ListLength))) +\n  geom_point(data = graphRT, mapping = aes(x = procPos, y = rt, color = ListLength), size = 2, alpha = 1 / 3, position = dodge) +\n  geom_line(data = graphRT, mapping = aes(x = procPos, y = rt, colour = ListLength, group = interaction(procPeriod, ListLength)), position = dodge) +\n  geom_errorbar(aes(ymin = rt - se, ymax = rt + se, colour = ListLength), width = .15, position = position_dodge(.9)) +\n  facet_grid(exp ~ recallamount, scales = \"free\") +\n  scale_colour_manual(values = viridisLite::viridis(8, direction = -1), name = \"List Length\") +\n  xlab(\"Serial Position\") +\n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  # ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1)) \n\n\n\n\n\n\n\n\n\nPlot RT collapsed over list-length and as a function of serial position relative to N. Position 0 means the 2nd item in recall2 condition or the 4th item in the recall4 condition. Separately for both experiments\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- procTrim %&gt;%\n  filter(procPeriod == \"Load\") %&gt;%\n  summarySEwithin(measurevar = \"rt\", withinvars = c(\"recallamount\", \"serial_position_rel\"), betweenvars = \"exp\", idvar = \"participant\") %&gt;%\n  mutate(serial_position_rel = as.numeric(as.character(serial_position_rel)))\n\n\nAutomatically converting the following non-factors to factors: exp, serial_position_rel\n\n\nCode\nggplot(graphRT, aes(serial_position_rel, rt)) +\n  geom_point(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color = as.factor(recallamount), group = as.factor(recallamount)), size = 2, alpha = 1 / 3) +\n  geom_line(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color = as.factor(recallamount), group = as.factor(recallamount))) +\n  geom_errorbar(aes(ymin = rt - se, ymax = rt + se, color = as.factor(recallamount), group = as.factor(recallamount)), width = .15) +\n  scale_colour_manual(values = viridisLite::viridis(2, direction = -1), name = \"List Length\") +\n  xlab(\"Relative Serial Position\") +\n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  # ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1)) +\n  facet_grid(exp ~ ., scales = \"free\") +\n  geom_vline(xintercept = 0.5, color = \"red\")\n\n\n\n\n\n\n\n\n\nSplit based on sample source\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- procTrim %&gt;% \n  filter(procPeriod == \"Load\") %&gt;% \n  summarySEwithin(measurevar = \"rt\", withinvars = c(\"recallamount\", \"serial_position_rel\"), betweenvars=c(\"exp\",\"sample\"), idvar=\"participant\") %&gt;% \n  mutate(serial_position_rel = as.numeric(as.character(serial_position_rel)))\n\n\nAutomatically converting the following non-factors to factors: exp, sample, serial_position_rel\n\n\nCode\nggplot(graphRT, aes(serial_position_rel, rt)) + \n  geom_point(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount)), size = 2, alpha = 1/3) +\n  geom_line(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount))) + \n  geom_errorbar(aes(ymin=rt-se, ymax=rt+se, color=as.factor(recallamount), group=as.factor(recallamount)), width=.15)+\n  scale_colour_manual(values = viridisLite::viridis(2, direction = -1), name=\"List Length\") + \n  xlab(\"Relative Serial Position\") + \n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  #ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust=1))+\n  facet_grid(exp~sample, scales=\"free\") +\n  geom_vline(xintercept = 0.5, color=\"red\")\n\n\n\n\n\n\n\n\n\nPlot RT collapsed over list-length and experiment, as a function of serial position relative to N. Position 0 means the 2nd item in recall2 condition or the 4th item in the recall4 condition.\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- procTrim %&gt;% \n  filter(procPeriod == \"Load\") %&gt;% \n  summarySEwithin(measurevar = \"rt\", withinvars = c(\"recallamount\", \"serial_position_rel\"), idvar=\"participant\") %&gt;% \n  mutate(serial_position_rel = as.numeric(as.character(serial_position_rel)))\n\n\nAutomatically converting the following non-factors to factors: serial_position_rel\n\n\nCode\nggplot(graphRT, aes(serial_position_rel, rt)) + \n  geom_point(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount)), size = 2, alpha = 1/3) +\n  geom_line(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount))) + \n  geom_errorbar(aes(ymin=rt-se, ymax=rt+se, color=as.factor(recallamount), group=as.factor(recallamount)), width=.15)+\n  scale_colour_manual(values = viridisLite::viridis(2, direction = -1), name=\"List Length\") + \n  xlab(\"Relative Serial Position\") + \n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  #ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust=1))+\n  geom_vline(xintercept = 0.5, color=\"red\")",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E1a and E1b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-1ab-Ven.html#traditional-lmer-analysis-for-quick-fit",
    "href": "docs/archive/JointAnalyses-1ab-Ven.html#traditional-lmer-analysis-for-quick-fit",
    "title": "Joint analysis of E1a and E1b",
    "section": "Traditional lmer analysis for quick fit",
    "text": "Traditional lmer analysis for quick fit\n\n\nCode\nif (!file.exists(R_output_file)) {\n  mldat &lt;- filter(procTrim, procPeriod == \"Load\", serial_position_rel &gt;= 1) %&gt;% \n    mutate(ListLength = as.numeric(as.character(ListLength)))\n  \n  ml1 &lt;- lmer(rt ~ sample + exp + ListLength + recallamount + (recallamount*serial_position_rel||participant), data=mldat)\n  ml2 &lt;- lmer(rt ~ sample + exp + ListLength + recallamount + serial_position_rel + (recallamount*serial_position_rel||participant), data=mldat)\n  ml3 &lt;- lmer(rt ~ sample + exp + ListLength + recallamount + recallamount*serial_position_rel + (recallamount*serial_position_rel||participant), data=mldat)\n}\n\nanova(ml1,ml2,ml3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: mldat\nModels:\nml1: log(rt) ~ sample + exp + ListLength + recallamount + ((1 | participant) + (0 + recallamount | participant) + (0 + serial_position_rel | participant) + (0 + recallamount:serial_position_rel | participant))\nml2: log(rt) ~ sample + exp + ListLength + recallamount + serial_position_rel + ((1 | participant) + (0 + recallamount | participant) + (0 + serial_position_rel | participant) + (0 + recallamount:serial_position_rel | participant))\nml3: log(rt) ~ sample + exp + ListLength + recallamount + recallamount * serial_position_rel + ((1 | participant) + (0 + recallamount | participant) + (0 + serial_position_rel | participant) + (0 + recallamount:serial_position_rel | participant))\n    npar   AIC   BIC  logLik deviance Chisq Df Pr(&gt;Chisq)    \nml1   14 15222 15335 -7597.2    15194                        \nml2   15 15215 15336 -7592.7    15185  9.03  1   0.002656 ** \nml3   16 15202 15330 -7584.9    15170 15.55  1  8.036e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nsummary(ml2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(rt) ~ sample + exp + ListLength + recallamount + serial_position_rel +  \n    ((1 | participant) + (0 + recallamount | participant) + (0 +  \n        serial_position_rel | participant) + (0 + recallamount:serial_position_rel |          participant))\n   Data: mldat\n\nREML criterion at convergence: 15227.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1276 -0.6093 -0.1113  0.5045  5.2728 \n\nRandom effects:\n Groups        Name                              Variance  Std.Dev. Corr \n participant   (Intercept)                       0.0472203 0.21730       \n participant.1 recallamount2                     0.0034342 0.05860       \n               recallamount4                     0.0294753 0.17168  -0.43\n participant.2 serial_position_rel               0.0001665 0.01290       \n participant.3 recallamount2:serial_position_rel 0.0000000 0.00000       \n               recallamount4:serial_position_rel 0.0009663 0.03109   NaN \n Residual                                        0.1104201 0.33230       \nNumber of obs: 22365, groups:  participant, 120\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)          6.5773717  0.0476529 138.027\nsamplestudents       0.1108894  0.0505699   2.193\nexp1b               -0.5117154  0.0416599 -12.283\nListLength          -0.0001424  0.0016343  -0.087\nrecallamount4        0.1839298  0.0200426   9.177\nserial_position_rel  0.0062013  0.0020342   3.049\n\nCorrelation of Fixed Effects:\n            (Intr) smplst exp1b  LstLng rcllm4\nsamplstdnts -0.761                            \nexp1b       -0.303 -0.161                     \nListLength  -0.193  0.000 -0.001              \nrecallamnt4 -0.072  0.000 -0.001 -0.076       \nsrl_pstn_rl  0.006  0.000  0.001 -0.382  0.120\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nCode\nsummary(ml3)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(rt) ~ sample + exp + ListLength + recallamount + recallamount *  \n    serial_position_rel + ((1 | participant) + (0 + recallamount |  \n    participant) + (0 + serial_position_rel | participant) +      (0 + recallamount:serial_position_rel | participant))\n   Data: mldat\n\nREML criterion at convergence: 15216.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1249 -0.6100 -0.1096  0.5015  5.2442 \n\nRandom effects:\n Groups        Name                              Variance  Std.Dev. Corr \n participant   (Intercept)                       2.549e-02 0.159670      \n participant.1 recallamount2                     2.480e-02 0.157477      \n               recallamount4                     5.193e-02 0.227872 0.50 \n participant.2 serial_position_rel               8.367e-05 0.009147      \n participant.3 recallamount2:serial_position_rel 1.250e-04 0.011182      \n               recallamount4:serial_position_rel 7.099e-04 0.026644 -0.25\n Residual                                        1.103e-01 0.332186      \nNumber of obs: 22365, groups:  participant, 120\n\nFixed effects:\n                                    Estimate Std. Error t value\n(Intercept)                        6.5837464  0.0476067 138.295\nsamplestudents                     0.1120987  0.0505134   2.219\nexp1b                             -0.5143840  0.0416130 -12.361\nListLength                        -0.0001431  0.0016337  -0.088\nrecallamount4                      0.1581450  0.0211093   7.492\nserial_position_rel                0.0042292  0.0021835   1.937\nrecallamount4:serial_position_rel  0.0174225  0.0045932   3.793\n\nCorrelation of Fixed Effects:\n            (Intr) smplst exp1b  LstLng rcllm4 srl_p_\nsamplstdnts -0.761                                   \nexp1b       -0.303 -0.161                            \nListLength  -0.193  0.000 -0.001                     \nrecallamnt4 -0.074  0.000 -0.001 -0.071              \nsrl_pstn_rl -0.003  0.001  0.000 -0.356  0.185       \nrcllmnt4:__  0.034  0.000  0.001 -0.001 -0.322 -0.407\noptimizer (nloptwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n\n\n\nBRMS ANALYSES\nFit the preregistered BRMS models.\nModel 1 is the model that predicts no increase after position N+1 Model 2 predicts an equal increase after position N+1 for both recall2 and recall4 condition Model 3 includes an interactions between relative position and recall2 vs recall4\nFirst, to use the default priors, we need to transform the DV and IVS. DV is log transformed because RTs are skewed, and then z transformed. Continuous predictiors are also z transformed, while categorical predictions are set to sum contrasts.\n\n\nCode\nif (!file.exists(R_output_file)) {\n  bmldat &lt;- mldat %&gt;% \n    mutate(rt = scale(log(rt))[,1],\n           ListLength = scale(ListLength)[,1],\n           serial_position_rel = scale(serial_position_rel)[,1],\n           exp = as.factor(exp))\n  \n  contrasts(bmldat$recallamount) &lt;- \"contr.sum\"\n  contrasts(bmldat$exp) &lt;- \"contr.sum\"\n  \n  \n  bml1r &lt;- brm(rt ~ exp + ListLength + recallamount + (recallamount*serial_position_rel||participant), data=bmldat, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  \n  bml2r &lt;- brm(rt ~ exp + ListLength + recallamount + serial_position_rel + (recallamount*serial_position_rel||participant), data=bmldat, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  \n  bml3r &lt;- brm(rt ~ exp + ListLength + recallamount + serial_position_rel*recallamount + (recallamount*serial_position_rel||participant), data=bmldat, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n}\n\n\nView the estimates of the full model:\n\n\nCode\nsummary(bml3r)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: rt ~ exp + ListLength + recallamount + serial_position_rel * recallamount + (recallamount * serial_position_rel || participant) \n   Data: bmldat (Number of observations: 22365) \n  Draws: 4 chains, each with iter = 20000; warmup = 10000; thin = 1;\n         total post-warmup draws = 40000\n\nMultilevel Hyperparameters:\n~participant (Number of levels: 120) \n                                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                             0.49      0.03     0.43     0.56 1.00     6317    12031\nsd(recallamount1)                         0.22      0.02     0.19     0.26 1.00     9276    15119\nsd(serial_position_rel)                   0.04      0.01     0.02     0.05 1.00     9781    11736\nsd(recallamount1:serial_position_rel)     0.04      0.01     0.02     0.05 1.00     8196     7753\n\nRegression Coefficients:\n                                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                             0.11      0.04     0.03     0.20 1.00     2525     5213\nexp1                                  0.54      0.05     0.45     0.63 1.00     2598     5596\nListLength                           -0.00      0.01    -0.01     0.01 1.00    66163    30894\nrecallamount1                        -0.21      0.02    -0.25    -0.17 1.00     5608    10754\nserial_position_rel                   0.04      0.01     0.03     0.06 1.00    46471    31699\nrecallamount1:serial_position_rel    -0.03      0.01    -0.04    -0.01 1.00    49485    31639\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.67      0.00     0.67     0.68 1.00    69962    28168\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompute bayes factors via bridge sampling:\n\n\nCode\nset.seed(5236)\n\nif (!file.exists(R_output_file)) {\n  BF21r &lt;- bayes_factor(bml2r, bml1r)\n  BF32r &lt;- bayes_factor(bml3r, bml2r)\n}\n\nBF21r\n\n\nEstimated Bayes factor in favor of bml2r over bml1r: 155.88635\n\n\nCode\nBF32r\n\n\nEstimated Bayes factor in favor of bml3r over bml2r: 35.47096\n\n\nNow, let’s run models separately for the recall2 and recall4 conditions, so that we know whether we have evidence specifically for or against increase within each condition\n\n\nCode\nif (!file.exists(R_output_file)) {\n  bmldat2 &lt;- filter(bmldat, recallamount == 2)\n  bmldat4 &lt;- filter(bmldat, recallamount == 4)\n  \n  \n  bmlr_r2_1 &lt;- brm(rt ~ exp + ListLength + (serial_position_rel|participant), data=bmldat2, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  bmlr_r2_2 &lt;- brm(rt ~ exp + ListLength + serial_position_rel + (serial_position_rel|participant), data=bmldat2, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  \n  bmlr_r4_1 &lt;- brm(rt ~ exp + ListLength + (serial_position_rel|participant), data=bmldat4, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  bmlr_r4_2 &lt;- brm(rt ~ exp + ListLength + serial_position_rel + (serial_position_rel|participant), data=bmldat4, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n\n}\n\n\nCompute bayes factors separately for within recall2 and recall4 condition via bridge sampling:\n\n\nCode\nset.seed(5236)\n\nif (!file.exists(R_output_file)) {\n  BFr_r2_2v1 &lt;- bayes_factor(bmlr_r2_2, bmlr_r2_1)\n  BFr_r4_2v1 &lt;- bayes_factor(bmlr_r4_2, bmlr_r4_1)\n}\n\nBFr_r2_2v1\n\n\nEstimated Bayes factor in favor of bmlr_r2_2 over bmlr_r2_1: 0.23612\n\n\nCode\nBFr_r4_2v1\n\n\nEstimated Bayes factor in favor of bmlr_r4_2 over bmlr_r4_1: 181.59623\n\n\n\nOLD ANALYSES WITHOUT RANDOM SLOPES EFFECTS\n\n\nCode\nif (!file.exists(R_output_file)) {\n  bmldat &lt;- mldat %&gt;% \n    mutate(rt = scale(log(rt))[,1],\n           ListLength = scale(ListLength)[,1],\n           serial_position_rel = scale(serial_position_rel)[,1],\n           exp = as.factor(exp))\n  \n  contrasts(bmldat$recallamount) &lt;- \"contr.sum\"\n  contrasts(bmldat$exp) &lt;- \"contr.sum\"\n  \n  \n  bml1 &lt;- brm(rt ~ exp + ListLength + recallamount + (1|participant), data=bmldat, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  bml2 &lt;- brm(rt ~ exp + ListLength + recallamount + serial_position_rel + (1|participant), data=bmldat, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  bml3 &lt;- brm(rt ~ exp + ListLength + recallamount + serial_position_rel*recallamount + (1|participant), data=bmldat, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n}\n\n\nView the estimates of the full model:\n\n\nCode\nsummary(bml3)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: rt ~ exp + ListLength + recallamount + serial_position_rel * recallamount + (1 | participant) \n   Data: bmldat (Number of observations: 22365) \n  Draws: 4 chains, each with iter = 20000; warmup = 10000; thin = 1;\n         total post-warmup draws = 40000\n\nMultilevel Hyperparameters:\n~participant (Number of levels: 120) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.48      0.03     0.42     0.54 1.00     2190     4496\n\nRegression Coefficients:\n                                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                             0.10      0.04     0.01     0.18 1.00      778     1445\nexp1                                  0.52      0.04     0.43     0.61 1.01      726     1733\nListLength                           -0.00      0.01    -0.01     0.01 1.00    25820    29486\nrecallamount1                        -0.20      0.01    -0.21    -0.19 1.00    23963    27801\nserial_position_rel                   0.04      0.01     0.02     0.05 1.00    18054    24498\nrecallamount1:serial_position_rel    -0.02      0.01    -0.03    -0.01 1.00    23181    27603\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.70      0.00     0.69     0.71 1.00    35381    30595\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompute bayes factors via bridge sampling:\n\n\nCode\nset.seed(5236)\n\nif (!file.exists(R_output_file)) {\n  BF21 &lt;- bayes_factor(bml2, bml1)\n  BF32 &lt;- bayes_factor(bml3, bml2)\n}\n\nBF21\n\n\nEstimated Bayes factor in favor of bml2 over bml1: 37.70294\n\n\nCode\nBF32\n\n\nEstimated Bayes factor in favor of bml3 over bml2: 11.91208\n\n\nNow, let’s run models separately for the recall2 and recall4 conditions, so that we know whether we have evidence specifically for or against increase within each condition\n\n\nCode\nif (!file.exists(R_output_file)) {\n  bmldat2 &lt;- filter(bmldat, recallamount == 2)\n  bmldat4 &lt;- filter(bmldat, recallamount == 4)\n  \n  \n  bml_r2_1 &lt;- brm(rt ~ exp + ListLength + (1|participant), data=bmldat2, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  bml_r2_2 &lt;- brm(rt ~ exp + ListLength + serial_position_rel + (1|participant), data=bmldat2, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  \n  bml_r4_1 &lt;- brm(rt ~ exp + ListLength + (1|participant), data=bmldat4, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n  bml_r4_2 &lt;- brm(rt ~ exp + ListLength + serial_position_rel + (1|participant), data=bmldat4, \n             save_all_pars = TRUE, iter = 20000, cores = 4, chains = 4)\n\n}\n\n\n\n\nCode\nset.seed(5236)\n\nif (!file.exists(R_output_file)) {\n  BF_r2_2v1 &lt;- bayes_factor(bml_r2_2, bml_r2_1)\n  BF_r4_2v1 &lt;- bayes_factor(bml_r4_2, bml_r4_1)\n}\n\nBF_r2_2v1\n\n\nEstimated Bayes factor in favor of bml_r2_2 over bml_r2_1: 0.60844\n\n\nCode\nBF_r4_2v1\n\n\nEstimated Bayes factor in favor of bml_r4_2 over bml_r4_1: 358.33508\n\n\nsave the estimated models to avoid fitting them again in the future\n\n\nCode\n# save.image(here::here('output/exp1_updated.RData'))",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E1a and E1b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-2ab-Ven.html",
    "href": "docs/archive/JointAnalyses-2ab-Ven.html",
    "title": "Joint analysis of E2a and E2b",
    "section": "",
    "text": "Note\n\n\n\nThese are legacy analyses scripts from my older project structure. The scripts are kept here for reference and reproducibility purposes until I completely transition to the new structure. I want to replicate the analysis in the new project structure to make sure everything is working as expected. Nowadays I also have better solutions than if(file.exists()) for managing the data and model fits, so I will update the script accordingly.",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E2a and E2b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-2ab-Ven.html#load-and-clean-data",
    "href": "docs/archive/JointAnalyses-2ab-Ven.html#load-and-clean-data",
    "title": "Joint analysis of E2a and E2b",
    "section": "Load and clean data",
    "text": "Load and clean data\nJoint analysis of Experiment 2a and 2b. Load libraries:\n\n\nCode\nlibrary(lme4)\nlibrary(Rmisc)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(brms)\n\ndodge &lt;- position_dodge(1)\ntheme_set(theme_dark(base_size = 11))\n\n\nLoad and combine data that was preprocessed with David’s script.\n\n\nCode\n# if the script has been run before, preload the data and model fits\nR_output_file &lt;- here::here('output/exp2.RData')\n\nif (!file.exists(R_output_file)) {\n  exp2a &lt;- read.csv(here::here('data/Expmt2a-PreReg/Processed/LongFormAnalysisData.csv'))\n  exp2b &lt;- read.csv(here::here('data/Expmt2b-PreReg/Processed/LongFormAnalysisData.csv'))\n  exp2a$exp &lt;- '2a'\n  exp2b$exp &lt;- '2b'\n  exp2b$participant &lt;- exp2b$participant\n  dat &lt;- bind_rows(exp2a,exp2b)\n  \n  # Apply David's trimming script\n  procTrim &lt;- dat %&gt;% \n    filter(ProportionTonesCorrect == 1) %&gt;% \n    mutate(rt = rt*1000) %&gt;%  # Make RTs milliseconds\n    group_by(participant) %&gt;% # grouping by participant so SD per participant calculated - similar to perParticipant = True in Trimr\n    filter(rt &gt;= 200)%&gt;%      # remove anything &lt;200ms as this likely means they were anticipating response rather than reacting - same as Joseph & Morey 2021\n    filter(rt &lt; 5000) %&gt;%       \n    filter(abs(rt - mean(rt))/sd(rt) &lt;= 3) \n  \n  # This just gives a quick summary count of amount of data before and after trimming.\n  nrow(dat) #original number\n  nrow(procTrim) #number remaining\n  nrow(dat) - nrow(procTrim) #number removed  \n  \n  ##Rename variables to align with previous research\n  procTrim[procTrim == \"LeadTone_RT1\"] &lt;- \"Lead-in-4\"\n  procTrim[procTrim == \"LeadTone_RT2\"] &lt;- \"Lead-in-3\"\n  procTrim[procTrim == \"LeadTone_RT3\"] &lt;- \"Lead-in-2\"\n  procTrim[procTrim == \"LeadTone_RT4\"] &lt;- \"Lead-in-1\"\n  procTrim[procTrim == \"MainTone_RT1\"] &lt;- \"List-1\"\n  procTrim[procTrim == \"MainTone_RT2\"] &lt;- \"List-2\"\n  procTrim[procTrim == \"MainTone_RT3\"] &lt;- \"List-3\"\n  procTrim[procTrim == \"MainTone_RT4\"] &lt;- \"List-4\"\n  procTrim[procTrim == \"MainTone_RT5\"] &lt;- \"List-5\"\n  procTrim[procTrim == \"MainTone_RT6\"] &lt;- \"List-6\"\n  procTrim[procTrim == \"MainTone_RT7\"] &lt;- \"List-7\"\n  procTrim[procTrim == \"MainTone_RT8\"] &lt;- \"List-8\"\n  procTrim[procTrim == \"MainTone_RT9\"] &lt;- \"List-9\"\n  \n  procTrim &lt;- procTrim %&gt;%  \n    mutate(procPeriod = case_when(grepl(\"Lead-in\", procPos) ~ \"Lead-in\",\n                                  grepl(\"List\", procPos) ~\"Load\")) %&gt;% \n    mutate(procPos = factor(procPos, levels = c(\"Lead-in-4\", \"Lead-in-3\", \"Lead-in-2\", \"Lead-in-1\", \"List-1\", \"List-2\", \"List-3\", \"List-4\", \"List-5\", \"List-6\", \"List-7\", \"List-8\", \"List-9\"))) %&gt;%\n    mutate(ListLength = as.factor(ListLength))%&gt;%\n    mutate(serial_position_abs = as.numeric(gsub('List-','', procPos)),\n           serial_position_rel = serial_position_abs-recallamount) %&gt;% \n    mutate(recallamount = as.factor(recallamount)) %&gt;% \n    ungroup()\n} else {\n  load(R_output_file)\n  R_output_file &lt;- here::here('output/exp2.RData')\n}",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E2a and E2b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-2ab-Ven.html#data-visualisation",
    "href": "docs/archive/JointAnalyses-2ab-Ven.html#data-visualisation",
    "title": "Joint analysis of E2a and E2b",
    "section": "Data Visualisation",
    "text": "Data Visualisation\nWe can now plot our tidied and trimmed data. We will relabel some variables and then calculate the summary statistics before plotting.\nReproduce the basic plot for both experiments (code from David’s script)\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- summarySEwithin(ungroup(procTrim), measurevar = \"rt\", withinvars = c(\"ListLength\",\"recallamount\", \"procPos\", \"procPeriod\"), betweenvars=\"exp\", idvar=\"participant\")\n\n\nAutomatically converting the following non-factors to factors: exp, procPeriod\n\n\nCode\nggplot(graphRT, aes(procPos, rt, group=interaction(procPeriod, ListLength))) + \n  geom_point(data = graphRT, mapping = aes(x = procPos, y = rt, color = ListLength), size = 2, alpha = 1/3, position = dodge) +\n  geom_line(data = graphRT, mapping = aes(x = procPos, y = rt, colour = ListLength, group = interaction(procPeriod, ListLength)), position = dodge) + \n  geom_errorbar(aes(ymin=rt-se, ymax=rt+se, colour = ListLength), width=.15, position=position_dodge(.9))+\n  facet_grid(exp~recallamount, scales=\"free\") + \n  scale_colour_manual(values = viridisLite::viridis(8, direction = -1), name=\"List Length\") + \n  xlab(\"Serial Position\") + \n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  #ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust=1)) \n\n\n\n\n\n\n\n\n\nPlot RT collapsed over list-length and as a function of serial position relative to N. Position 0 means the 2nd item in recall2 condition or the 4th item in the recall4 condition. Separately for both experiments\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- procTrim %&gt;% \n  filter(procPeriod == \"Load\") %&gt;% \n  summarySEwithin(measurevar = \"rt\", withinvars = c(\"recallamount\", \"serial_position_rel\"), betweenvars=\"exp\", idvar=\"participant\") %&gt;% \n  mutate(serial_position_rel = as.numeric(as.character(serial_position_rel)))\n\n\nAutomatically converting the following non-factors to factors: exp, serial_position_rel\n\n\nCode\nggplot(graphRT, aes(serial_position_rel, rt)) + \n  geom_point(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount)), size = 2, alpha = 1/3) +\n  geom_line(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount))) + \n  geom_errorbar(aes(ymin=rt-se, ymax=rt+se, color=as.factor(recallamount), group=as.factor(recallamount)), width=.15)+\n  scale_colour_manual(values = viridisLite::viridis(2, direction = -1), name=\"List Length\") + \n  xlab(\"Relative Serial Position\") + \n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  #ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust=1))+\n  facet_grid(exp~., scales=\"free\") +\n  geom_vline(xintercept = 0.5, color=\"red\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- procTrim %&gt;% \n  filter(procPeriod == \"Load\") %&gt;% \n  summarySEwithin(measurevar = \"rt\", withinvars = c(\"recallamount\", \"serial_position_rel\", \"which_n_first\"), betweenvars=\"exp\", idvar=\"participant\") %&gt;% \n  mutate(serial_position_rel = as.numeric(as.character(serial_position_rel)))\n\n\nAutomatically converting the following non-factors to factors: exp, serial_position_rel, which_n_first\n\n\nCode\nggplot(graphRT, aes(serial_position_rel, rt)) + \n  geom_point(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount)), size = 2, alpha = 1/3) +\n  geom_line(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount))) + \n  geom_errorbar(aes(ymin=rt-se, ymax=rt+se, color=as.factor(recallamount), group=as.factor(recallamount)), width=.15)+\n  scale_colour_manual(values = viridisLite::viridis(2, direction = -1), name=\"List Length\") + \n  xlab(\"Relative Serial Position\") + \n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  #ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust=1))+\n  facet_grid(exp~which_n_first, scales=\"free\") +\n  geom_vline(xintercept = 0.5, color=\"red\")\n\n\n\n\n\n\n\n\n\nPlot RT collapsed over list-length and experiment, as a function of serial position relative to N. Position 0 means the 2nd item in recall2 condition or the 4th item in the recall4 condition.\n\n\nCode\n# Means for plot, with within-participant SEMs\ngraphRT &lt;- procTrim %&gt;% \n  filter(procPeriod == \"Load\") %&gt;% \n  summarySEwithin(measurevar = \"rt\", withinvars = c(\"recallamount\", \"serial_position_rel\"), idvar=\"participant\") %&gt;% \n  mutate(serial_position_rel = as.numeric(as.character(serial_position_rel)))\n\n\nAutomatically converting the following non-factors to factors: serial_position_rel\n\n\nCode\nggplot(graphRT, aes(serial_position_rel, rt)) + \n  geom_point(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount)), size = 2, alpha = 1/3) +\n  geom_line(data = graphRT, mapping = aes(x = serial_position_rel, y = rt, color=as.factor(recallamount), group=as.factor(recallamount))) + \n  geom_errorbar(aes(ymin=rt-se, ymax=rt+se, color=as.factor(recallamount), group=as.factor(recallamount)), width=.15)+\n  scale_colour_manual(values = viridisLite::viridis(2, direction = -1), name=\"List Length\") + \n  xlab(\"Relative Serial Position\") + \n  ylab(\"Mean Response Time (ms)\") +\n  # scale_y_continuous(breaks=seq(600, 1400, 50), limits = c(600, 1400))+\n  #ylim(250, 800) +\n  theme(axis.text.x = element_text(angle = 70, hjust=1))+\n  geom_vline(xintercept = 0.5, color=\"red\")",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E2a and E2b"
    ]
  },
  {
    "objectID": "docs/archive/JointAnalyses-2ab-Ven.html#traditional-lmer-analysis-for-quick-fit",
    "href": "docs/archive/JointAnalyses-2ab-Ven.html#traditional-lmer-analysis-for-quick-fit",
    "title": "Joint analysis of E2a and E2b",
    "section": "Traditional lmer analysis for quick fit",
    "text": "Traditional lmer analysis for quick fit\n\n\nCode\nif (!file.exists(R_output_file)) {\n  mldat &lt;- filter(procTrim, procPeriod == \"Load\", serial_position_rel &gt;= 1) %&gt;% \n    mutate(ListLength = as.numeric(as.character(ListLength)))\n  \n  ml1 &lt;- lmer(rt ~ exp + ListLength + recallamount + (1|participant), data=mldat)\n  ml2 &lt;- lmer(rt ~ exp + ListLength + recallamount + serial_position_rel + (1|participant), data=mldat)\n  ml3 &lt;- lmer(rt ~ exp + ListLength + recallamount + recallamount*serial_position_rel + (1|participant), data=mldat)\n}\n\nanova(ml1, ml2, ml3)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: mldat\nModels:\nml1: rt ~ exp + ListLength + recallamount + (1 | participant)\nml2: rt ~ exp + ListLength + recallamount + serial_position_rel + (1 | participant)\nml3: rt ~ exp + ListLength + recallamount + recallamount * serial_position_rel + (1 | participant)\n    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nml1    6 206384 206429 -103186   206372                         \nml2    7 206329 206382 -103157   206315 56.765  1  4.912e-14 ***\nml3    8 206326 206387 -103155   206310  4.902  1    0.02682 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nsummary(ml2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rt ~ exp + ListLength + recallamount + serial_position_rel +      (1 | participant)\n   Data: mldat\n\nREML criterion at convergence: 206283.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.3355 -0.4196 -0.1080  0.2537  7.9642 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 180716   425.1   \n Residual                 99315   315.1   \nNumber of obs: 14339, groups:  participant, 120\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          948.1679    56.6966  16.724\nexp2b               -400.0325    77.8066  -5.141\nListLength            -0.9874     2.1615  -0.457\nrecallamount4         13.6958     5.5575   2.464\nserial_position_rel   16.8199     2.2304   7.541\n\nCorrelation of Fixed Effects:\n            (Intr) exp2b  LstLng rcllm4\nexp2b       -0.686                     \nListLength  -0.221 -0.001              \nrecallamnt4 -0.004  0.001 -0.198       \nsrl_pstn_rl  0.038  0.000 -0.509  0.199\n\n\n\n\nCode\nsummary(ml3)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rt ~ exp + ListLength + recallamount + recallamount * serial_position_rel +      (1 | participant)\n   Data: mldat\n\nREML criterion at convergence: 206273.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.3202 -0.4185 -0.1087  0.2524  7.9618 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 180676   425.1   \n Residual                 99288   315.1   \nNumber of obs: 14339, groups:  participant, 120\n\nFixed effects:\n                                   Estimate Std. Error t value\n(Intercept)                        940.7041    56.7904  16.564\nexp2b                             -400.1547    77.7980  -5.144\nListLength                          -0.9887     2.1612  -0.457\nrecallamount4                       35.1675    11.1779   3.146\nserial_position_rel                 19.7417     2.5914   7.618\nrecallamount4:serial_position_rel   -9.0990     4.1100  -2.214\n\nCorrelation of Fixed Effects:\n            (Intr) exp2b  LstLng rcllm4 srl_p_\nexp2b       -0.685                            \nListLength  -0.221 -0.001                     \nrecallamnt4 -0.053  0.000 -0.099              \nsrl_pstn_rl  0.003  0.000 -0.438  0.527       \nrcllmnt4:__  0.059  0.001  0.000 -0.868 -0.509\n\n\n\nBRMS ANALYSES\nFit the preregistered BRMS models.\nModel 1 is the model that predicts no increase after position N+1 Model 2 predicts an equal increase after position N+1 for both recall2 and recall4 condition Model 3 includes an interactions between relative position and recall2 vs recall4\nFirst, to use the default priors, we need to transform the DV and IVS. DV is log transformed because RTs are skewed, and then z transformed. Continuous predictiors are also z transformed, while categorical predictions are set to sum contrasts.\n\n\nCode\nif (!file.exists(R_output_file)) {\n  bmldat &lt;- mldat %&gt;% \n    mutate(rt = scale(log(rt))[,1],\n           ListLength = scale(ListLength)[,1],\n           serial_position_rel = scale(serial_position_rel)[,1],\n           exp = as.factor(exp))\n  \n  contrasts(bmldat$recallamount) &lt;- \"contr.sum\"\n  contrasts(bmldat$exp) &lt;- \"contr.sum\"\n  \n  \n  bml1 &lt;- brm(rt ~ exp + ListLength + recallamount + (1|participant), data=bmldat, \n             save_all_pars = TRUE, iter = 40000, cores = 4, chains = 4)\n  bml2 &lt;- brm(rt ~ exp + ListLength + recallamount + serial_position_rel + (1|participant), data=bmldat, \n             save_all_pars = TRUE, iter = 40000, cores = 4, chains = 4)\n  bml3 &lt;- brm(rt ~ exp + ListLength + recallamount + serial_position_rel*recallamount + (1|participant), data=bmldat, \n             save_all_pars = TRUE, iter = 40000, cores = 4, chains = 4)\n}\n\n\nView the estimates of the full model:\n\n\nCode\nsummary(bml3)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: rt ~ exp + ListLength + recallamount + serial_position_rel * recallamount + (1 | participant) \n   Data: bmldat (Number of observations: 14339) \n  Draws: 4 chains, each with iter = 40000; warmup = 20000; thin = 1;\n         total post-warmup draws = 80000\n\nMultilevel Hyperparameters:\n~participant (Number of levels: 120) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.66      0.04     0.58     0.75 1.00     2694     6009\n\nRegression Coefficients:\n                                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                             0.07      0.06    -0.05     0.19 1.00      796     1884\nexp1                                  0.50      0.06     0.38     0.62 1.01      817     1791\nListLength                           -0.00      0.01    -0.01     0.01 1.00    29036    47530\nrecallamount1                        -0.01      0.01    -0.02     0.00 1.00    35887    48612\nserial_position_rel                   0.05      0.01     0.03     0.06 1.00    27690    44198\nrecallamount1:serial_position_rel     0.01      0.01     0.00     0.02 1.00    35557    49379\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.58      0.00     0.58     0.59 1.00    40467    50082\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompute bayes factors via bridge sampling:\n\n\nCode\nset.seed(5236)\n\nif (!file.exists(R_output_file)) {\n  BF21 &lt;- bayes_factor(bml2, bml1)\n  BF32 &lt;- bayes_factor(bml3, bml2)\n}\n\nBF21\n\n\nEstimated Bayes factor in favor of bml2 over bml1: 522765483334965.93750\n\n\nCode\nBF32\n\n\nEstimated Bayes factor in favor of bml3 over bml2: 0.20207\n\n\n\n\nCode\n# save.image(here::here('output/exp2.RData'))",
    "crumbs": [
      "Home",
      "Archive",
      "Joint analysis of E2a and E2b"
    ]
  }
]